library(htm2txt)
install.packages('html2txt')
install.packages('htm2txt')
library(htm2txt)
url <- 'https://www.bbc.com/news/world-us-canada-66089626'
text <- gettxt(url)
guard_url <- 'https://www.theguardian.com/education/2023/aug/11/college-legacy-admissions-affirmative-action-democrats'
nyp_url <- 'https://nypost.com/2023/07/04/harvard-sued-over-overwhelmingly-white-legacy-admissions/'
bbc <- gettxt(bbc_url)
bbc_url <- 'https://www.bbc.com/news/world-us-canada-66089626'
bbc <- gettxt(bbc_url)
guard <- gettxt(guard_url)
nyp <- gettext(nyp_url)
library(tm);library(stringr);library(tidytext);library(tidyverse);library(SnowballC);library(widyr);library(ggplot2);library(igraph);library(ggraph);library(stringi);library(pacman);library(textstem);library(qdapRegex)
for(doc01 in c(bbc, guard, nyp)){
# nothing after readLines, reads it in as paragraphs
#doc01 <-paste(readLines(file))
#cleaning document
doc01 <- gsub(pattern="\\W" , replace = " ", doc01)%>% gsub(pattern = "\\d", replacement = " ", doc01)
doc01<-tolower(doc01)
#doc01<- lemmatize_strings(doc01)
#doc01 <- rm_nchar_words(doc01, "1,3")
# doc01 <- gsub('\\b\\w{1,3}\\b','',doc01)
doc01 <- stripWhitespace(doc01)
doc01 <- stringi::stri_remove_empty(doc01) # remove blank paragraphs
# doc01 <- doc01 %>% anti_join(my_stop_words)
# doc01<- removeWords(doc01, c(stopwords("english"),"bolc","tmts","cohen","hambrick",
#                              "ancona","ruth","ross","johnson","rodriguez","ibid","adrp","also","bunnell",
#                              "cajina","rc","ccc","ile","ar","finkelstein","capl","brown","christiansen",
#                              "fema","kingston","mcpherson","opms","hqda","arng","wocs","capi","opmg",
#                              "much", "will", "grinston", "however", "rater", "find", "make", "table")) # adding capital T to the stopwords
cleaned <- cleaned %>% append(doc01) # object for cleaned paragraph text
names <- names %>% append(rep(file, length(doc01))) # object for the document name each paragraph belongs to
}
sentiment_df <- tibble(doc = c(seq(1:3)), text = c(bbc, guard, nyp), document = c("bbc", "guard", "nyp"))
sentiment_paragraphs <- sentiment_df %>% unnest_tokens(word, text) %>% count(word, paragraph, sort = TRUE)
sentiment_paragraphs <- sentiment_df %>% unnest_tokens(word, text) %>% count(word, doc, sort = TRUE)
sentiment_words <- sentiment_df %>% unnest_tokens(word,text) %>% count(word,document, sort = TRUE)
View(sentiment_words)
tfidf_words <-pull(sentiment_words, word)
sentimentWords <- get_nrc_sentiment(tfidf_words)
library(Dict); library(topicmodels);library(tidytext);library(ggplot2);library(dplyr);library(tidyr);library(widyr);library(igraph);library(ggraph);library(forcats);library(syuzhet); library(cowplot)
tfidf_words <-pull(sentiment_words, word)
sentimentWords <- get_nrc_sentiment(tfidf_words)
install.packages('Dict')
library(Dict); library(topicmodels);library(tidytext);library(ggplot2);library(dplyr);library(tidyr);library(widyr);library(igraph);library(ggraph);library(forcats);library(syuzhet); library(cowplot)
install.packages('topicmodels')
library(Dict); library(topicmodels);library(tidytext);library(ggplot2);library(dplyr);library(tidyr);library(widyr);library(igraph);library(ggraph);library(forcats);library(syuzhet); library(cowplot)
install.packages('igraph')
library(Dict); library(topicmodels);library(tidytext);library(ggplot2);library(dplyr);library(tidyr);library(widyr);library(igraph);library(ggraph);library(forcats);library(syuzhet); library(cowplot)
install.packages('syuzhet')
library(Dict); library(topicmodels);library(tidytext);library(ggplot2);library(dplyr);library(tidyr);library(widyr);library(ggraph);library(forcats);library(syuzhet); library(cowplot)
tfidf_words <-pull(sentiment_words, word)
sentimentWords <- get_nrc_sentiment(tfidf_words)
library(syuzhet)
sentimentWords <- get_nrc_sentiment(tfidf_words)
View(sentimentWords)
View(sentiment_words)
bbc_sentiment <- sentiment_words %>% filter(doc == "bbc") %>% pull(word)
bbc_sentiment <- sentiment_words %>% filter(document == "bbc") %>% pull(word)
bbc_sentiment <- get_nrc_sentiment(bbc_sentiment)
sum(bbc_sentiment)
mean(bbc_sentiment)
plot(
bbc_sentiment,
type="l",
main="Example Plot Trajectory",
xlab = "Narrative Time",
ylab= "Emotional Valence"
)
bbc_sentiment <- get_nrc_sentiment(bbc)
sum(bbc_sentiment)
mean(bbc_sentiment)
plot(
bbc_sentiment,
type="l",
main="Example Plot Trajectory",
xlab = "Narrative Time",
ylab= "Emotional Valence"
)
View(bbc_sentiment)
s_v <- get_sentences(bbc)
s_v_sentiment <- get_sentiment(s_v)
plot(
s_v_sentiment,
type="l",
main="Example Plot Trajectory",
xlab = "Narrative Time",
ylab= "Emotional Valence"
)
bbc_sentiment <- sentiment_words %>% filter(document == "nyp") %>% pull(word)
bbc_sentiment <- get_nrc_sentiment(nyp)
sum(bbc_sentiment)
s_v <- get_sentences(nyp)
s_v_sentiment <- get_sentiment(s_v)
plot(
s_v_sentiment,
type="l",
main="Example Plot Trajectory",
xlab = "Narrative Time",
ylab= "Emotional Valence"
)
s_v <- get_sentences(nyp)
s_v_sentiment <- get_sentiment(s_v)
plot(
s_v_sentiment,
type="l",
main="Example Plot Trajectory",
xlab = "Narrative Time",
ylab= "Emotional Valence"
)
nyp_url <- 'https://nypost.com/2023/07/04/harvard-sued-over-overwhelmingly-white-legacy-admissions/'
nyp <- gettext(nyp_url)
nyp
nyp_url <- 'https://nypost.com/2023/07/04/harvard-sued-over-overwhelmingly-white-legacy-admissions'
nyp <- gettext(nyp_url)
bbc_sentiment <- sentiment_words %>% filter(document == "guard") %>% pull(word)
bbc_sentiment <- get_nrc_sentiment(guard)
sum(bbc_sentiment)
s_v <- get_sentences(guard)
s_v_sentiment <- get_sentiment(s_v)
plot(
s_v_sentiment,
type="l",
main="Example Plot Trajectory",
xlab = "Narrative Time",
ylab= "Emotional Valence"
)
